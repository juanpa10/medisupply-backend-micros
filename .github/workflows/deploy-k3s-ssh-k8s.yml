name: Deploy to Kubernetes (k3s via SSH)

on:
  workflow_run:
    workflows: ["Build & Push to ECR (Kubernetes)"]   # debe coincidir EXACTO con el nombre del workflow de build
    types: [completed]
  repository_dispatch: # dispatch que se usa desde el repo del frontend 
    types: [deploy-from-front] 
  workflow_dispatch: {}

permissions:
  contents: read

jobs:
  deploy:
    if: ${{ github.event_name == 'repository_dispatch' || (github.event.workflow_run.conclusion == 'success' && github.event.workflow_run.head_branch == 'main') }}
    runs-on: ubuntu-latest
    # Give the deploy job more headroom to run long remote commands (installing k3s, waiting jobs/rollouts)
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - name: Collect files (deploy-medisupply/*)
        run: |
          rm -rf out && mkdir -p out
          if [ -d deploy-medisupply ]; then
            cp -r deploy-medisupply out/deploy-medisupply
          fi
          # evita "tar: empty archive" si no hay nada que copiar
          if [ ! -d out/deploy-medisupply ]; then
            echo "placeholder" > out/.keep
          fi

      - name: Clean remote deploy dir
        uses: appleboy/ssh-action@v1.2.0
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_KEY }}
          script_stop: true
          script: |
            # debug: print early connectivity info and enable shell trace
            set -x
            echo "SSH connected (clean dir): $(whoami)@$(hostname)"; date
            sudo rm -rf /tmp/medisupply
            mkdir -p /tmp/medisupply

      - name: Copy files to server
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_KEY }}
          source: "out"                 # importante: copia el dir, no "out/*"
          target: "/tmp/medisupply"

      - name: Install k3s & deploy
        uses: appleboy/ssh-action@v1.2.0
        env:
          AWS_REGION: ${{ vars.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
          ECR_REPO: ${{ vars.ECR_REPOSITORY }}
          TAG_PREFIX: k8s-
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_KEY }}
          script_stop: true
          envs: AWS_REGION,AWS_ACCOUNT_ID,ECR_REPO,TAG_PREFIX
          script: |
            # debug: print early connectivity info and enable shell trace
            set -x
            echo "SSH connected (install/deploy): $(whoami)@$(hostname)"; date; uname -a; id
            BASE="/tmp/medisupply/out/deploy-medisupply"

            # helper: wait for kube-apiserver to respond via k3s kubectl
            wait_for_kube() {
              NAMESPACE=medisupply
              ATTEMPTS=60
              SLEEP=5
              echo "Waiting for k3s kubectl to be ready (up to $((ATTEMPTS*SLEEP))s)"
              for i in $(seq 1 $ATTEMPTS); do
                # use a short timeout wrapper so the command can't hang the SSH action
                if timeout 5s sudo k3s kubectl version --short >/dev/null 2>&1; then
                  echo "k3s kubectl is available"
                  return 0
                fi
                echo "  k3s not ready yet ($i/$ATTEMPTS), sleeping $SLEEP..."
                sleep $SLEEP
              done
              echo "k3s kubectl did not become ready in time; dumping diagnostics"
              sudo systemctl status k3s || true
              sudo journalctl -u k3s --no-pager -n 200 || true
              sudo k3s kubectl get pods -A -o wide || true
              sudo k3s kubectl get events -A --sort-by=.lastTimestamp || true
              return 1
            }
            BASE="/tmp/medisupply/out/deploy-medisupply"

            # instala k3s si subiste el script
            if [ -f "$BASE/scripts/k3s-install.sh" ]; then
              sudo bash "$BASE/scripts/k3s-install.sh"
            fi
            # after installing k3s, ensure the kube API is responsive before running kubectl
            if ! wait_for_kube; then
              echo "kube API is not responsive after k3s install; aborting deploy"
              exit 1
            fi

            # aplica manifests base si existen
            if [ -d "$BASE/k8s" ]; then
              [ -f "$BASE/k8s/namespace.yaml" ]        && sudo k3s kubectl apply -f "$BASE/k8s/namespace.yaml" || true
              [ -f "$BASE/k8s/configmap.env.yaml" ]    && sudo k3s kubectl apply -f "$BASE/k8s/configmap.env.yaml" || true
              if [ -d "$BASE/k8s/db" ]; then
                [ -f "$BASE/k8s/db/secret.yaml" ]      && sudo k3s kubectl apply -f "$BASE/k8s/db/secret.yaml" || true
                [ -f "$BASE/k8s/db/statefulset.yaml" ] && sudo k3s kubectl apply -f "$BASE/k8s/db/statefulset.yaml" || true
                [ -f "$BASE/k8s/db/service.yaml" ]     && sudo k3s kubectl apply -f "$BASE/k8s/db/service.yaml" || true
              fi
              [ -f "$BASE/k8s/ingress.yaml" ]          && sudo k3s kubectl apply -f "$BASE/k8s/ingress.yaml" || true
            fi

            # ECR imagePullSecret (idempotente)
            PASS=$(aws ecr get-login-password --region "$AWS_REGION")
            sudo k3s kubectl -n medisupply delete secret ecr-cred --ignore-not-found
            sudo k3s kubectl -n medisupply create secret docker-registry ecr-cred \
              --docker-server="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com" \
              --docker-username="AWS" \
              --docker-password="$PASS"

            # SHA del build que terminó
            SHA="${{ github.event.workflow_run.head_sha }}"

            # >>> FRONT: manejo de deploy cuando viene desde el repo del front
            EVENT="${{ github.event_name }}"
            if [ "$EVENT" = "repository_dispatch" ]; then
              echo "== Deploy triggered from FRONT (repository_dispatch) =="

              # Rutas base (ya definiste BASE arriba)
              K8S="$BASE/k8s"

              # Aplica manifests del front (no rompe si ya existen)
              [ -f "$K8S/web/service.yaml" ]    && sudo k3s kubectl apply -f "$K8S/web/service.yaml"    || true
              [ -f "$K8S/web/deployment.yaml" ] && sudo k3s kubectl apply -f "$K8S/web/deployment.yaml" || true

              # Calcula la imagen del front: usa el SHA enviado; si no vino, usa latest
              WEB_SHA="${{ github.event.client_payload.web_sha || '' }}"

              if [ -n "$WEB_SHA" ]; then
                WEB_TAG="k8s-web-${WEB_SHA}"
              else
                WEB_TAG="k8s-web-latest"
              fi
              
              echo "Using web image tag: $WEB_TAG"
              WEB_IMG="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO:${WEB_TAG}"

              # Actualiza el deployment del front
              sudo k3s kubectl -n medisupply set image deploy/web-portal web-portal="$WEB_IMG" || true
              # wait for rollout with longer timeout and a couple of retries to handle transient kube API issues
              for i in $(seq 1 3); do
                if sudo k3s kubectl -n medisupply rollout status deploy/web-portal --timeout=600s; then
                  break
                fi
                echo "Web rollout attempt $i failed; gathering debug info"
                sudo k3s kubectl -n medisupply describe deploy/web-portal || true
                sudo k3s kubectl -n medisupply get pods -o wide || true
                # events and previous logs to help diagnose transient failures
                sudo k3s kubectl -n medisupply get events --sort-by=.lastTimestamp || true
                sudo k3s kubectl -n medisupply get pods -o name | xargs -r -n1 -I{} sudo k3s kubectl -n medisupply logs {} --all-containers --previous || true
                if [ "$i" -lt 3 ]; then
                  echo "Retrying rollout in 10s..."
                  sleep 10
                else
                  echo "Web rollout failed after 3 attempts"
                fi
              done || true
              echo "Front updated to image: $WEB_IMG"

              # Salir aquí para no correr el bucle de micros del back
              exit 0
            fi
            # <<< FRONT

            # despliegue dinámico: cada carpeta de servicio (excepto web, db y yamls sueltos)
            if [ -d "$BASE/k8s" ]; then
              for d in $(ls -1 "$BASE/k8s"); do
                if [ "$d" = "web" ] || [ "$d" = "db" ] || [ "$d" = "namespace.yaml" ] || [ "$d" = "configmap.env.yaml" ] || [ "$d" = "ingress.yaml" ]; then
                  continue
                fi
                if [ -d "$BASE/k8s/$d" ] && [ -f "$BASE/k8s/$d/service.yaml" ] && [ -f "$BASE/k8s/$d/deployment.yaml" ]; then
                  echo ">> Deploy service folder: $d"

                  # --- overrides opcionales ---
                  IMG_NAME="$d"   # por defecto usamos nombre de carpeta para imagen
                  if [ -f "$BASE/k8s/$d/image.txt" ]; then
                    IMG_NAME="$(tr -d ' \n\r' < "$BASE/k8s/$d/image.txt")"
                  fi

                  DEPLOY_NAME="$d"  # por defecto el Deployment se llama como la carpeta
                  if [ -f "$BASE/k8s/$d/deploy.name" ]; then
                    DEPLOY_NAME="$(tr -d ' \n\r' < "$BASE/k8s/$d/deploy.name")"
                  fi

                  echo "   ECR image tag base: ${TAG_PREFIX}${IMG_NAME}-${SHA}"
                  echo "   K8s deployment:     ${DEPLOY_NAME}"

                  # espera imagen en ECR con el nombre correcto
                  for i in $(seq 1 30); do
                    ok=$(aws ecr describe-images \
                        --repository-name "$ECR_REPO" \
                        --image-ids imageTag="${TAG_PREFIX}${IMG_NAME}-${SHA}" \
                        --query 'imageDetails[0].imageDigest' \
                        --output text 2>/dev/null || true)
                    if [ -n "$ok" ] && [ "$ok" != "None" ]; then
                      echo "   ✔ image ${TAG_PREFIX}${IMG_NAME}-${SHA} found"
                      break
                    fi
                    echo "   …waiting image ${TAG_PREFIX}${IMG_NAME}-${SHA} ($i/30)"; sleep 4
                  done

                  # aplica manifests y cambia imagen del deployment real
                  sudo k3s kubectl apply -f "$BASE/k8s/$d/service.yaml"
                  sudo k3s kubectl apply -f "$BASE/k8s/$d/deployment.yaml"

                  # If there's an init-job.yaml, replace placeholder image and run the Job
                  if [ -f "$BASE/k8s/$d/init-job.yaml" ]; then
                    echo "   Found init-job for $d, applying with image tag"
                    TMP_JOB="/tmp/${d}-init-job.yaml"
                    cp "$BASE/k8s/$d/init-job.yaml" "$TMP_JOB"
                    sed -i "s|REPLACED_BY_PIPELINE|$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO:${TAG_PREFIX}${IMG_NAME}-${SHA}|g" "$TMP_JOB"
                    # Remove any previous job with same name to avoid immutable field errors
                    sudo k3s kubectl -n medisupply delete job ${d}-init-db --ignore-not-found || true
                    sudo k3s kubectl -n medisupply delete pods -l job-name=${d}-init-db --ignore-not-found || true
                    sudo k3s kubectl -n medisupply apply -f "$TMP_JOB"
                    # wait for job to succeed (timeout ~600s). Detect failures early and dump debug info.
                    for i in $(seq 1 120); do
                      succeeded=$(sudo k3s kubectl -n medisupply get job ${d}-init-db -o jsonpath='{.status.succeeded}' 2>/dev/null || echo "0")
                      failed=$(sudo k3s kubectl -n medisupply get job ${d}-init-db -o jsonpath='{.status.failed}' 2>/dev/null || echo "0")
                      if [ "$succeeded" = "1" ]; then
                        echo "   Init job ${d}-init-db succeeded"
                        break
                      fi
                      if [ "$failed" != "0" ] && [ "$failed" != "" ]; then
                        echo "   Init job ${d}-init-db reported failures (failed=${failed}), dumping describe/logs"
                        sudo k3s kubectl -n medisupply describe job ${d}-init-db || true
                        sudo k3s kubectl -n medisupply logs job/${d}-init-db || true
                        # also list pods related to the job and dump previous logs and events
                        sudo k3s kubectl -n medisupply get pods --selector=job-name=${d}-init-db -o wide || true
                        sudo k3s kubectl -n medisupply get pods --selector=job-name=${d}-init-db -o name | xargs -r -n1 -I{} sudo k3s kubectl -n medisupply logs {} --all-containers --previous || true
                        sudo k3s kubectl -n medisupply get events --sort-by=.lastTimestamp || true
                        exit 1
                      fi
                      echo "   Waiting for init job ${d}-init-db to complete ($i/120)"; sleep 5
                    done
                    # if loop finished without success, gather diagnostics
                    succeeded=$(sudo k3s kubectl -n medisupply get job ${d}-init-db -o jsonpath='{.status.succeeded}' 2>/dev/null || echo "0")
                    if [ "$succeeded" != "1" ]; then
                      echo "   Init job ${d}-init-db did not complete in time, dumping describe/logs"
                      sudo k3s kubectl -n medisupply describe job ${d}-init-db || true
                      sudo k3s kubectl -n medisupply logs job/${d}-init-db || true
                      sudo k3s kubectl -n medisupply get pods --selector=job-name=${d}-init-db -o wide || true
                      sudo k3s kubectl -n medisupply get pods --selector=job-name=${d}-init-db -o name | xargs -r -n1 -I{} sudo k3s kubectl -n medisupply logs {} --all-containers --previous || true
                      sudo k3s kubectl -n medisupply get events --sort-by=.lastTimestamp || true
                      exit 1
                    fi
                    echo "   Showing logs for job ${d}-init-db (if any):"
                    sudo k3s kubectl -n medisupply logs job/${d}-init-db || true
                    # cleanup temporary job YAML
                    rm -f "$TMP_JOB" || true
                  fi

                  CNAME=$(sudo k3s kubectl -n medisupply get deploy/${DEPLOY_NAME} -o jsonpath='{.spec.template.spec.containers[0].name}')
                  IMG="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO:${TAG_PREFIX}${IMG_NAME}-${SHA}"
                  sudo k3s kubectl -n medisupply set image deploy/${DEPLOY_NAME} $CNAME=$IMG
                  # wait for rollout with longer timeout and retries; on final failure dump describe/pods for debugging
                  for i in $(seq 1 3); do
                    if sudo k3s kubectl -n medisupply rollout status deploy/${DEPLOY_NAME} --timeout=600s; then
                      break
                    fi
                    echo "Rollout attempt $i for ${DEPLOY_NAME} failed; gathering debug info"
                    sudo k3s kubectl -n medisupply describe deploy/${DEPLOY_NAME} || true
                    # try to list pods filtered by common label 'app' then fallback to all pods and grep
                    sudo k3s kubectl -n medisupply get pods --selector=app=${DEPLOY_NAME} -o wide || sudo k3s kubectl -n medisupply get pods -o wide | grep ${DEPLOY_NAME} || true
                    # events and previous logs for deployment pods
                    sudo k3s kubectl -n medisupply get events --sort-by=.lastTimestamp || true
                    sudo k3s kubectl -n medisupply get pods --selector=app=${DEPLOY_NAME} -o name | xargs -r -n1 -I{} sudo k3s kubectl -n medisupply logs {} --all-containers --previous || true
                    if [ "$i" -lt 3 ]; then
                      echo "Retrying rollout for ${DEPLOY_NAME} in 10s..."
                      sleep 10
                    else
                      echo "Rollout for ${DEPLOY_NAME} failed after 3 attempts"
                      # exit non-zero so CI knows deployment didn't converge
                      exit 1
                    fi
                  done
                fi
              done
            fi
